{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data, cat_cols=None, output_col=None):\n",
    " \n",
    "\n",
    "        self.n = data.shape[0]\n",
    "\n",
    "        if output_col:\n",
    "            self.y = data[output_col].astype(np.float32).values.reshape(-1, 1)\n",
    "        else:\n",
    "            self.y =  np.zeros((self.n, 1))\n",
    "\n",
    "        self.cat_cols = cat_cols if cat_cols else []\n",
    "        self.cont_cols = [col for col in data.columns\n",
    "                          if col not in self.cat_cols + [output_col]]\n",
    "\n",
    "        if self.cont_cols:\n",
    "            self.cont_X = data[self.cont_cols].astype(np.float32).values\n",
    "        else:\n",
    "            self.cont_X = np.zeros((self.n, 1))\n",
    "\n",
    "        if self.cat_cols:\n",
    "            self.cat_X = data[cat_cols].astype(np.int64).values\n",
    "        else:\n",
    "            self.cat_X =  np.zeros((self.n, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "    \"\"\"\n",
    "    Denotes the total number of samples.\n",
    "    \"\"\"\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "    Generates one sample of data.\n",
    "    \"\"\"\n",
    "        return [self.y[idx], self.cont_X[idx], self.cat_X[idx]]\n",
    "\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dims, no_of_cont, lin_layer_sizes,\n",
    "               output_size, emb_dropout, lin_layer_dropouts):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    emb_dims: List of two element tuples\n",
    "      This list will contain a two element tuple for each\n",
    "      categorical feature. The first element of a tuple will\n",
    "      denote the number of unique values of the categorical\n",
    "      feature. The second element will denote the embedding\n",
    "      dimension to be used for that feature.\n",
    "    no_of_cont: Integer\n",
    "      The number of continuous features in the data.\n",
    "    lin_layer_sizes: List of integers.\n",
    "      The size of each linear layer. The length will be equal\n",
    "      to the total number\n",
    "      of linear layers in the network.\n",
    "    output_size: Integer\n",
    "      The size of the final output.\n",
    "    emb_dropout: Float\n",
    "      The dropout to be used after the embedding layers.\n",
    "    lin_layer_dropouts: List of floats\n",
    "      The dropouts to be used after each linear layer.\n",
    "    \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers\n",
    "        self.emb_layers = nn.ModuleList([nn.Embedding(x, y)\n",
    "                                         for x, y in emb_dims])\n",
    "\n",
    "        no_of_embs = sum([y for x, y in emb_dims])\n",
    "        self.no_of_embs = no_of_embs\n",
    "        self.no_of_cont = no_of_cont\n",
    "\n",
    "        # Linear Layers\n",
    "        first_lin_layer = nn.Linear(self.no_of_embs + self.no_of_cont,\n",
    "                                    lin_layer_sizes[0])\n",
    "\n",
    "        self.lin_layers =\\\n",
    "         nn.ModuleList([first_lin_layer] +\\\n",
    "              [nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1])\n",
    "               for i in range(len(lin_layer_sizes) - 1)])\n",
    "\n",
    "        for lin_layer in self.lin_layers:\n",
    "            nn.init.kaiming_normal_(lin_layer.weight.data)\n",
    "\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(lin_layer_sizes[-1],\n",
    "                                      output_size)\n",
    "        nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
    "\n",
    "        # Batch Norm Layers\n",
    "        self.first_bn_layer = nn.BatchNorm1d(self.no_of_cont)\n",
    "        self.bn_layers = nn.ModuleList([nn.BatchNorm1d(size)\n",
    "                                        for size in lin_layer_sizes])\n",
    "\n",
    "        # Dropout Layers\n",
    "        self.emb_dropout_layer = nn.Dropout(emb_dropout)\n",
    "        self.droput_layers = nn.ModuleList([nn.Dropout(size)\n",
    "                                      for size in lin_layer_dropouts])\n",
    "\n",
    "    def forward(self, cont_data, cat_data):\n",
    "\n",
    "        if self.no_of_embs != 0:\n",
    "            x = [emb_layer(cat_data[:, i])\n",
    "               for i,emb_layer in enumerate(self.emb_layers)]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_dropout_layer(x)\n",
    "\n",
    "        if self.no_of_cont != 0:\n",
    "            normalized_cont_data = self.first_bn_layer(cont_data)\n",
    "\n",
    "            if self.no_of_embs != 0:\n",
    "                x = torch.cat([x, normalized_cont_data], 1) \n",
    "            else:\n",
    "                x = normalized_cont_data\n",
    "\n",
    "        for lin_layer, dropout_layer, bn_layer in\\\n",
    "            zip(self.lin_layers, self.droput_layers, self.bn_layers):\n",
    "\n",
    "            x = F.relu(lin_layer(x))\n",
    "            x = bn_layer(x)\n",
    "            x = dropout_layer(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date_in'] = pd.to_datetime(data['date_in'])\n",
    "data['year'] = data['date_in'].map(lambda x: x.strftime('%Y'))\n",
    "data['month'] = data['date_in'].map(lambda x: x.strftime('%m'))\n",
    "data['day'] = data['date_in'].map(lambda x: x.strftime('%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>agency_id</th>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>532</td>\n",
       "      <td>588</td>\n",
       "      <td>588</td>\n",
       "      <td>588</td>\n",
       "      <td>588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis_water_real</th>\n",
       "      <td>0.261</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis_shopping</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_bedrooms</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_persons</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house_size</th>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>land_size</th>\n",
       "      <td>726</td>\n",
       "      <td>726</td>\n",
       "      <td>726</td>\n",
       "      <td>726</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>build_year</th>\n",
       "      <td>1953</td>\n",
       "      <td>1953</td>\n",
       "      <td>1953</td>\n",
       "      <td>1953</td>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>renovation_year</th>\n",
       "      <td>2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apartment</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indoor_pool</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spa</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>internet</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pets_allowed</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water_view</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire_stove</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agency_rating</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>08</td>\n",
       "      <td>08</td>\n",
       "      <td>08</td>\n",
       "      <td>08</td>\n",
       "      <td>08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0      1      2      3      4\n",
       "agency_id           90     90     90     90     90\n",
       "price              532    588    588    588    588\n",
       "dis_water_real   0.261  0.261  0.261  0.261  0.261\n",
       "dis_shopping         3      3      3      3      3\n",
       "no_bedrooms          3      3      3      3      3\n",
       "max_persons          4      4      4      4      4\n",
       "house_size         140    140    140    140    140\n",
       "land_size          726    726    726    726    726\n",
       "build_year        1953   1953   1953   1953   1953\n",
       "renovation_year   2014   2014   2014   2014   2014\n",
       "apartment            0      0      0      0      0\n",
       "indoor_pool          0      0      0      0      0\n",
       "spa                  1      1      1      1      1\n",
       "internet             1      1      1      1      1\n",
       "pets_allowed         0      0      0      0      0\n",
       "water_view           1      1      1      1      1\n",
       "fire_stove           1      1      1      1      1\n",
       "agency_rating        3      3      3      3      3\n",
       "year              2016   2016   2016   2016   2016\n",
       "month               08     08     08     08     08\n",
       "day                 27     26     25     24     23"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr = ['date_in','house_pk']\n",
    "\n",
    "data = data.drop(dr,axis = 1)\n",
    "data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['agency_id', 'apartment', 'indoor_pool', 'spa', 'internet', 'pets_allowed', 'water_view', 'fire_stove', 'year', 'month', 'day']\n",
    "output_columns = 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85195, 21)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}\n",
    "for cat_col in categorical_features:\n",
    "        label_encoders[cat_col] = LabelEncoder()\n",
    "        data[cat_col] = label_encoders[cat_col].fit_transform(data[cat_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TabularDataset(data=data, cat_cols=categorical_features, output_col=output_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "dataloader = DataLoader(dataset, batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 2, 2, 2, 2, 2, 2, 4, 12, 31]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_dims = [int(data[col].nunique()) for col in categorical_features]\n",
    "cat_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 2),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (4, 2),\n",
       " (12, 6),\n",
       " (31, 16)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dims = [(x, min(50, (x + 1) // 2)) for x in cat_dims]\n",
    "emb_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedForwardNN(emb_dims, no_of_cont=9, lin_layer_sizes=[50, 100],\n",
    "                          output_size=1, emb_dropout=0.04,\n",
    "                          lin_layer_dropouts=[0.001,0.01]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardNN(\n",
      "  (emb_layers): ModuleList(\n",
      "    (0): Embedding(4, 2)\n",
      "    (1): Embedding(2, 1)\n",
      "    (2): Embedding(2, 1)\n",
      "    (3): Embedding(2, 1)\n",
      "    (4): Embedding(2, 1)\n",
      "    (5): Embedding(2, 1)\n",
      "    (6): Embedding(2, 1)\n",
      "    (7): Embedding(2, 1)\n",
      "    (8): Embedding(4, 2)\n",
      "    (9): Embedding(12, 6)\n",
      "    (10): Embedding(31, 16)\n",
      "  )\n",
      "  (lin_layers): ModuleList(\n",
      "    (0): Linear(in_features=42, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=100, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (first_bn_layer): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn_layers): ModuleList(\n",
      "    (0): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (emb_dropout_layer): Dropout(p=0.04)\n",
      "  (droput_layers): ModuleList(\n",
      "    (0): Dropout(p=0.001)\n",
      "    (1): Dropout(p=0.01)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 372733.125\n",
      "loss: 414318.125\n",
      "loss: 450339.312\n",
      "loss: 371294.000\n",
      "loss: 298650.031\n",
      "loss: 380865.938\n",
      "loss: 413957.156\n",
      "loss: 428347.750\n",
      "loss: 339070.875\n",
      "loss: 311659.562\n",
      "loss: 294935.469\n",
      "loss: 200001.344\n",
      "loss: 235193.969\n",
      "loss: 152916.484\n",
      "loss: 143250.859\n",
      "loss: 217440.094\n",
      "loss: 156203.453\n",
      "loss: 130534.094\n",
      "loss: 132738.344\n",
      "loss: 56379.609\n",
      "loss: 81454.188\n",
      "loss: 39026.027\n",
      "loss: 76324.656\n",
      "loss: 22570.779\n",
      "loss: 33659.035\n",
      "loss: 56022.523\n",
      "loss: 30564.424\n",
      "loss: 39445.141\n",
      "loss: 48782.688\n",
      "loss: 33481.246\n",
      "loss: 49696.645\n",
      "loss: 64311.863\n",
      "loss: 48971.883\n",
      "loss: 41842.715\n",
      "loss: 31264.102\n",
      "loss: 34848.176\n",
      "loss: 25339.396\n",
      "loss: 31726.633\n",
      "loss: 21478.301\n",
      "loss: 17477.660\n",
      "loss: 40352.160\n",
      "loss: 25923.250\n",
      "loss: 31710.609\n",
      "loss: 42272.734\n",
      "loss: 14377.985\n",
      "loss: 31015.713\n",
      "loss: 27594.117\n",
      "loss: 29184.539\n",
      "loss: 56441.098\n",
      "loss: 35275.109\n",
      "loss: 57423.547\n",
      "loss: 36998.004\n",
      "loss: 57168.781\n",
      "loss: 25049.277\n",
      "loss: 35334.617\n",
      "loss: 47338.949\n",
      "loss: 31146.760\n",
      "loss: 22143.041\n",
      "loss: 20933.846\n",
      "loss: 26805.371\n",
      "loss: 44760.211\n",
      "loss: 28941.898\n",
      "loss: 19614.881\n",
      "loss: 25929.131\n",
      "loss: 18548.955\n",
      "loss: 31235.055\n",
      "loss: 38208.438\n",
      "loss: 28642.602\n",
      "loss: 30811.953\n",
      "loss: 20486.242\n",
      "loss: 17287.186\n",
      "loss: 18780.262\n",
      "loss: 22973.438\n",
      "loss: 54997.047\n",
      "loss: 27682.176\n",
      "loss: 13292.437\n",
      "loss: 19904.570\n",
      "loss: 11937.510\n",
      "loss: 27850.131\n",
      "loss: 55501.301\n",
      "loss: 36675.789\n",
      "loss: 24425.029\n",
      "loss: 18331.650\n",
      "loss: 22762.676\n",
      "loss: 14785.445\n",
      "loss: 27947.721\n",
      "loss: 26420.451\n",
      "loss: 59609.305\n",
      "loss: 23046.598\n",
      "loss: 19884.002\n",
      "loss: 28233.658\n",
      "loss: 21442.529\n",
      "loss: 18724.730\n",
      "loss: 16965.793\n",
      "loss: 19104.207\n",
      "loss: 22017.814\n",
      "loss: 26106.293\n",
      "loss: 21532.783\n",
      "loss: 21035.330\n",
      "loss: 51036.941\n",
      "loss: 15422.298\n",
      "loss: 36281.703\n",
      "loss: 33904.438\n",
      "loss: 16305.976\n",
      "loss: 25285.225\n",
      "loss: 44354.098\n",
      "loss: 21900.215\n",
      "loss: 20644.844\n",
      "loss: 37389.598\n",
      "loss: 28902.170\n",
      "loss: 19956.016\n",
      "loss: 28506.812\n",
      "loss: 36786.051\n",
      "loss: 30105.502\n",
      "loss: 53307.125\n",
      "loss: 22021.750\n",
      "loss: 42863.742\n",
      "loss: 22918.355\n",
      "loss: 23795.109\n",
      "loss: 41349.301\n",
      "loss: 22333.025\n",
      "loss: 23831.154\n",
      "loss: 35140.703\n",
      "loss: 22607.652\n",
      "loss: 13596.108\n",
      "loss: 54314.680\n",
      "loss: 14960.684\n",
      "loss: 15898.104\n",
      "loss: 26518.262\n",
      "loss: 18586.504\n",
      "loss: 19903.381\n",
      "loss: 35884.152\n",
      "loss: 21080.701\n",
      "loss: 28565.105\n",
      "loss: 13710.061\n",
      "loss: 38227.398\n",
      "loss: 25359.727\n",
      "loss: 18644.871\n",
      "loss: 36416.246\n",
      "loss: 15444.279\n",
      "loss: 25354.566\n",
      "loss: 25463.965\n",
      "loss: 10488.354\n",
      "loss: 29564.445\n",
      "loss: 18246.113\n",
      "loss: 29804.766\n",
      "loss: 57782.570\n",
      "loss: 21420.693\n",
      "loss: 18579.006\n",
      "loss: 18519.926\n",
      "loss: 31982.328\n",
      "loss: 22399.404\n",
      "loss: 19235.256\n",
      "loss: 17087.428\n",
      "loss: 16083.353\n",
      "loss: 41707.438\n",
      "loss: 38322.230\n",
      "loss: 19409.258\n",
      "loss: 33149.301\n",
      "loss: 26703.031\n",
      "loss: 20073.512\n",
      "loss: 31346.400\n",
      "loss: 41590.305\n",
      "loss: 21881.430\n",
      "loss: 20491.119\n",
      "loss: 19887.941\n",
      "loss: 15652.744\n",
      "loss: 32438.270\n",
      "loss: 23667.994\n",
      "loss: 14733.299\n",
      "loss: 31840.008\n",
      "loss: 32457.457\n",
      "loss: 20470.096\n",
      "loss: 19636.348\n",
      "loss: 30202.316\n",
      "loss: 62974.887\n",
      "loss: 13547.584\n",
      "loss: 20839.330\n",
      "loss: 22692.387\n",
      "loss: 25272.361\n",
      "loss: 12021.439\n",
      "loss: 29922.863\n",
      "loss: 34955.441\n",
      "loss: 26147.383\n",
      "loss: 20868.473\n",
      "loss: 25161.510\n",
      "loss: 41641.879\n",
      "loss: 26275.668\n",
      "loss: 35565.047\n",
      "loss: 16194.038\n",
      "loss: 16875.559\n",
      "loss: 18083.484\n",
      "loss: 34306.285\n",
      "loss: 20291.359\n",
      "loss: 9983.241\n",
      "loss: 29644.092\n",
      "loss: 22454.006\n",
      "loss: 16992.043\n",
      "loss: 15937.812\n",
      "loss: 32061.777\n",
      "loss: 28575.756\n",
      "loss: 20981.178\n",
      "loss: 21294.619\n",
      "loss: 30208.305\n",
      "loss: 26058.768\n",
      "loss: 17973.260\n",
      "loss: 25546.742\n",
      "loss: 35525.961\n",
      "loss: 14709.113\n",
      "loss: 21987.580\n",
      "loss: 20309.959\n",
      "loss: 15941.728\n",
      "loss: 14689.127\n",
      "loss: 33857.559\n",
      "loss: 23178.180\n",
      "loss: 28554.447\n",
      "loss: 18085.828\n",
      "loss: 17541.156\n",
      "loss: 36350.586\n",
      "loss: 21030.793\n",
      "loss: 26180.842\n",
      "loss: 16317.088\n",
      "loss: 15074.387\n",
      "loss: 32811.379\n",
      "loss: 30072.010\n",
      "loss: 13752.761\n",
      "loss: 15043.931\n",
      "loss: 16451.619\n",
      "loss: 17119.373\n",
      "loss: 11574.527\n",
      "loss: 26591.564\n",
      "loss: 26437.477\n",
      "loss: 47763.832\n",
      "loss: 40022.121\n",
      "loss: 16642.475\n",
      "loss: 14173.406\n",
      "loss: 25213.186\n",
      "loss: 49455.082\n",
      "loss: 11584.076\n",
      "loss: 24721.246\n",
      "loss: 19691.035\n",
      "loss: 11387.988\n",
      "loss: 32197.139\n",
      "loss: 34673.656\n",
      "loss: 16576.346\n",
      "loss: 39138.469\n",
      "loss: 17219.109\n",
      "loss: 22865.666\n",
      "loss: 23048.447\n",
      "loss: 9184.006\n",
      "loss: 27905.299\n",
      "loss: 19243.514\n",
      "loss: 51399.762\n",
      "loss: 12939.352\n",
      "loss: 11538.234\n",
      "loss: 13429.017\n",
      "loss: 33243.238\n",
      "loss: 22743.660\n",
      "loss: 14909.010\n",
      "loss: 21342.207\n",
      "loss: 15688.052\n",
      "loss: 13514.949\n",
      "loss: 46025.113\n",
      "loss: 17445.041\n",
      "loss: 13532.786\n",
      "loss: 33323.195\n",
      "loss: 16349.197\n",
      "loss: 20846.863\n",
      "loss: 20052.254\n",
      "loss: 17211.246\n",
      "loss: 24079.680\n",
      "loss: 30732.973\n",
      "loss: 21894.041\n",
      "loss: 23873.414\n",
      "loss: 18261.379\n",
      "loss: 20048.641\n",
      "loss: 17399.797\n",
      "loss: 19648.023\n",
      "loss: 15444.766\n",
      "loss: 21072.504\n",
      "loss: 15936.617\n",
      "loss: 23007.734\n",
      "loss: 18580.150\n",
      "loss: 11626.428\n",
      "loss: 17015.424\n",
      "loss: 21332.051\n",
      "loss: 13586.711\n",
      "loss: 74990.242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a757a3002e80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for y, cont_x, cat_x in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cont_x = cont_x.to(device)\n",
    "        cat_x = cat_x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        output = model(cont_x, cat_x)\n",
    "        \n",
    "        loss = criterion(output, y)\n",
    "        \n",
    "        print('loss: {:.3f}'.format(loss.item()))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
